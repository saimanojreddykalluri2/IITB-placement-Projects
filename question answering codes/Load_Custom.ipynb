{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Load_Custom_T2 CS772ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Word embedding\n",
        "inference\n"
      ],
      "metadata": {
        "id": "RTlfR0tU_cxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "zIb_bYEXIL3d",
        "outputId": "8ce2e45d-0046-402d-9fb0-e44b90e97edb",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, return_sequences=True, name=None, **kwargs):\n",
        "        super(Attention, self).__init__(name=name)\n",
        "        self.return_sequences = return_sequences\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "    \n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
        "                           initializer=\"glorot_uniform\", trainable=True)\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
        "                           initializer=\"glorot_uniform\", trainable=True)\n",
        "    \n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "    \n",
        "        e = tf.keras.activations.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
        "        a = tf.keras.activations.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "    \n",
        "        if self.return_sequences:\n",
        "            return a, output\n",
        "    \n",
        "        return a, tf.keras.backend.sum(output, axis=1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'return_sequences': self.return_sequences \n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "-LKC41cIrAOG",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:29.347887Z",
          "iopub.execute_input": "2022-04-01T04:35:29.348405Z",
          "iopub.status.idle": "2022-04-01T04:35:30.378232Z",
          "shell.execute_reply.started": "2022-04-01T04:35:29.348362Z",
          "shell.execute_reply": "2022-04-01T04:35:30.377485Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "# # Path to translation file\n",
        "# path_to_data = 'data/spa.txt'\n",
        "\n",
        "# # Read file\n",
        "# translation_file = open(path_to_data,\"r\", encoding='utf-8') \n",
        "# raw_data = translation_file.read()\n",
        "# translation_file.close()\n",
        "\n",
        "# # Parse data\n",
        "# raw_data = raw_data.split('\\n')\n",
        "# pairs = [sentence.split('\\t') for sentence in  raw_data]\n",
        "# pairs = pairs[1000:20000]"
      ],
      "metadata": {
        "id": "_GrpACeP08Zw",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:30.379673Z",
          "iopub.execute_input": "2022-04-01T04:35:30.379931Z",
          "iopub.status.idle": "2022-04-01T04:35:30.391658Z",
          "shell.execute_reply.started": "2022-04-01T04:35:30.379894Z",
          "shell.execute_reply": "2022-04-01T04:35:30.391029Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess data"
      ],
      "metadata": {
        "id": "3lHQ93PVEV2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOe2DHbIJ_QZ",
        "outputId": "e5d6027e-2186-4480-cf37-e4502d4ddbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sep_token=\" <pad> \"\n",
        "input_train_path=\"/content/drive/MyDrive/DLNLPA2/input_train.txt\"\n",
        "output_train_path=\"/content/drive/MyDrive/DLNLPA2/output_train.txt\"\n",
        "input_dev_path=\"/content/drive/MyDrive/DLNLPA2/input_dev.txt\"\n",
        "output_dev_path=\"/content/drive/MyDrive/DLNLPA2/output_dev.txt\"\n",
        "\n",
        "input_train_file = open(input_train_path,\"r\", encoding='utf-8')\n",
        "input_train_data = input_train_file.read()\n",
        "input_train_file.close()\n",
        "\n",
        "output_train_file = open(output_train_path,\"r\", encoding='utf-8')\n",
        "output_train_data = output_train_file.read()\n",
        "output_train_file.close()\n",
        "\n",
        "input_dev_file = open(input_dev_path,\"r\", encoding='utf-8')\n",
        "input_dev_data = input_dev_file.read()\n",
        "input_dev_file.close()\n",
        "\n",
        "output_dev_file = open(output_dev_path,\"r\", encoding='utf-8')\n",
        "output_dev_data = output_dev_file.read()\n",
        "output_dev_file.close()"
      ],
      "metadata": {
        "id": "uzsbxxhz3Zik",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:30.394339Z",
          "iopub.execute_input": "2022-04-01T04:35:30.394587Z",
          "iopub.status.idle": "2022-04-01T04:35:30.782514Z",
          "shell.execute_reply.started": "2022-04-01T04:35:30.394554Z",
          "shell.execute_reply": "2022-04-01T04:35:30.781791Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse data\n",
        "input_train_data = input_train_data.split('\\n')\n",
        "output_train_data = output_train_data.split('\\n')\n",
        "input_dev_data = input_dev_data.split('\\n')\n",
        "output_dev_data = output_dev_data.split('\\n')"
      ],
      "metadata": {
        "id": "CBZEKly86nTN",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:30.783925Z",
          "iopub.execute_input": "2022-04-01T04:35:30.784175Z",
          "iopub.status.idle": "2022-04-01T04:35:30.850043Z",
          "shell.execute_reply.started": "2022-04-01T04:35:30.784133Z",
          "shell.execute_reply": "2022-04-01T04:35:30.849318Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentence(sentence):\n",
        "    # Lower case the sentence\n",
        "    lower_case_sent = sentence.lower()\n",
        "    # Strip punctuation\n",
        "    string_punctuation = string.punctuation + \"¡\" + '¿'\n",
        "    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
        "   \n",
        "    return clean_sentence"
      ],
      "metadata": {
        "id": "3T7-tGg-5rT_",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:30.851801Z",
          "iopub.execute_input": "2022-04-01T04:35:30.852342Z",
          "iopub.status.idle": "2022-04-01T04:35:30.859366Z",
          "shell.execute_reply.started": "2022-04-01T04:35:30.852304Z",
          "shell.execute_reply": "2022-04-01T04:35:30.858692Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences):\n",
        "    # Create tokenizer\n",
        "    text_tokenizer = Tokenizer()\n",
        "    # Fit texts\n",
        "    text_tokenizer.fit_on_texts(sentences)\n",
        "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"
      ],
      "metadata": {
        "id": "QvrmdnKa5UzW",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:30.861004Z",
          "iopub.execute_input": "2022-04-01T04:35:30.861254Z",
          "iopub.status.idle": "2022-04-01T04:35:30.86687Z",
          "shell.execute_reply.started": "2022-04-01T04:35:30.861221Z",
          "shell.execute_reply": "2022-04-01T04:35:30.866134Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"# of Train Sentences: \",len(output_train_data))\n",
        "print(\"# of Dev Sentences: \",len(output_dev_data))"
      ],
      "metadata": {
        "id": "Pv_D2ZRP8U7P",
        "outputId": "cf821505-1b87-4820-9399-b746ec1f22e8",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:30.868394Z",
          "iopub.execute_input": "2022-04-01T04:35:30.868695Z",
          "iopub.status.idle": "2022-04-01T04:35:30.878022Z",
          "shell.execute_reply.started": "2022-04-01T04:35:30.868608Z",
          "shell.execute_reply": "2022-04-01T04:35:30.877226Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Train Sentences:  51035\n",
            "# of Dev Sentences:  3507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Seperate VOCAB\n",
        "#Combine train an dev\n",
        "input_data=input_train_data+input_dev_data\n",
        "output_data=output_train_data+output_dev_data\n",
        "\n",
        "# Clean sentences\n",
        "input = [clean_sentence(input) for input in input_data]\n",
        "output = [clean_sentence(output) for output in output_data]\n",
        "\n",
        "# Tokenize words\n",
        "input_tokenized, input_tokenizer = tokenize(input)\n",
        "output_tokenized, output_tokenizer = tokenize(output)\n",
        "\n",
        "print('Maximum length input sentence: {}'.format(len(max(input_tokenized,key=len))))\n",
        "print('Maximum length output sentence: {}'.format(len(max(output_tokenized,key=len))))\n",
        "\n",
        "\n",
        "# Check language length\n",
        "input_vocab = len(input_tokenizer.word_index) + 1\n",
        "output_vocab = len(output_tokenizer.word_index) + 1\n",
        "print(\"Input vocabulary is of {} unique words\".format(input_vocab))\n",
        "print(\"Output vocabulary is of {} unique words\".format(output_vocab))\n",
        "\n",
        "## Combined Vocab\n",
        "#Combine train an dev\n",
        "all_data=input_data+output_data\n",
        "\n",
        "# Clean sentences\n",
        "all = [clean_sentence(data) for data in all_data]\n",
        "\n",
        "# Tokenize words\n",
        "all_tokenized, all_tokenizer = tokenize(all)\n",
        "\n",
        "print('Maximum length of sentence: {}'.format(len(max(all_tokenized,key=len))))\n",
        "\n",
        "\n",
        "# Check language length\n",
        "vocab = len(all_tokenizer.word_index) + 1\n",
        "print(\"Vocabulary is of {} unique words\".format(vocab))\n"
      ],
      "metadata": {
        "id": "UTaRUvq66aRO",
        "outputId": "59795fcb-15ac-4075-d8bd-64f6423d6c37",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:30.879425Z",
          "iopub.execute_input": "2022-04-01T04:35:30.879696Z",
          "iopub.status.idle": "2022-04-01T04:35:52.321Z",
          "shell.execute_reply.started": "2022-04-01T04:35:30.87965Z",
          "shell.execute_reply": "2022-04-01T04:35:52.320237Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length input sentence: 664\n",
            "Maximum length output sentence: 5\n",
            "Input vocabulary is of 97594 unique words\n",
            "Output vocabulary is of 29529 unique words\n",
            "Maximum length of sentence: 664\n",
            "Vocabulary is of 97671 unique words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_len = int(len(max(input_tokenized,key=len)))\n",
        "max_output_len = int(len(max(output_tokenized,key=len)))\n",
        "\n",
        "input_pad_sentence = pad_sequences(input_tokenized, max_input_len, padding = \"post\")\n",
        "output_pad_sentence = pad_sequences(output_tokenized, max_output_len, padding = \"post\")\n",
        "\n",
        "# Reshape data\n",
        "input_pad_sentence = input_pad_sentence.reshape(*input_pad_sentence.shape, 1)\n",
        "output_pad_sentence = output_pad_sentence.reshape(*output_pad_sentence.shape, 1)"
      ],
      "metadata": {
        "id": "qo4q2umpDZAf",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:52.323526Z",
          "iopub.execute_input": "2022-04-01T04:35:52.32394Z",
          "iopub.status.idle": "2022-04-01T04:35:53.316971Z",
          "shell.execute_reply.started": "2022-04-01T04:35:52.323903Z",
          "shell.execute_reply": "2022-04-01T04:35:53.316187Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "WdcEEHOPEZCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = Input(shape=(max_input_len,))\n",
        "embedding = Embedding(input_dim=input_vocab, output_dim=128,)(input_sequence)\n",
        "encoder = Bidirectional(LSTM(64, return_sequences=False))(embedding)\n",
        "r_vec = RepeatVector(max_output_len)(encoder)\n",
        "a,att=Attention(return_sequences=True)(r_vec)\n",
        "decoder = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2))(att)\n",
        "logits = TimeDistributed(Dense(output_vocab))(decoder)"
      ],
      "metadata": {
        "id": "ONRr4W2XENHA",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:35:53.318241Z",
          "iopub.execute_input": "2022-04-01T04:35:53.318507Z",
          "iopub.status.idle": "2022-04-01T04:35:54.692395Z",
          "shell.execute_reply.started": "2022-04-01T04:35:53.318471Z",
          "shell.execute_reply": "2022-04-01T04:35:54.691698Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_dec_model = Model(input_sequence, Activation('softmax')(logits))\n",
        "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
        "              optimizer=Adam(1e-3),\n",
        "              metrics=['accuracy'])\n",
        "enc_dec_model.summary()"
      ],
      "metadata": {
        "id": "XS9GG4VTFdZE",
        "outputId": "ca7bc7b7-386a-4fec-c59e-c5333c41a39a",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:36:03.733614Z",
          "iopub.execute_input": "2022-04-01T04:36:03.734162Z",
          "iopub.status.idle": "2022-04-01T04:36:03.76035Z",
          "shell.execute_reply.started": "2022-04-01T04:36:03.734123Z",
          "shell.execute_reply": "2022-04-01T04:36:03.75962Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 664)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 664, 128)          12492032  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVect  (None, 5, 128)           0         \n",
            " or)                                                             \n",
            "                                                                 \n",
            " attention_5 (Attention)     ((None, 5, 1),            133       \n",
            "                              (None, 5, 128))                    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 5, 128)           98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 5, 29529)         3809241   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 5, 29529)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,499,038\n",
            "Trainable params: 16,499,038\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_pad_sentence=input_pad_sentence[:len(output_train_data)]\n",
        "train_output_pad_sentence=output_pad_sentence[:len(output_train_data)]"
      ],
      "metadata": {
        "id": "jOZ27OsiHaOR",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:36:09.77963Z",
          "iopub.execute_input": "2022-04-01T04:36:09.78035Z",
          "iopub.status.idle": "2022-04-01T04:36:09.784331Z",
          "shell.execute_reply.started": "2022-04-01T04:36:09.780309Z",
          "shell.execute_reply": "2022-04-01T04:36:09.783567Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_results = enc_dec_model.fit(train_input_pad_sentence, train_output_pad_sentence, batch_size=30, epochs=100)"
      ],
      "metadata": {
        "id": "mQHi1kLnGRwd",
        "execution": {
          "iopub.status.busy": "2022-04-01T04:50:39.740246Z",
          "iopub.execute_input": "2022-04-01T04:50:39.740727Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enc_dec_model.save(\"qa_without_embeddings.h5\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9osBZGbIJsw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "enc_dec_model = keras.models.load_model('/content/drive/MyDrive/DLNLPA2/qa_without_embeddings.h5', custom_objects={'Attention': Attention})"
      ],
      "metadata": {
        "id": "V9D-vl-jLg0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_sentence(logits, tokenizer):\n",
        "\n",
        "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '' \n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "# index = len(output_train_data)+10\n",
        "# print(\"The Input sentence is: {}\".format(input_data[index]))\n",
        "# print(\"The Output sentence is: {}\".format(output_data[index]))\n",
        "# print('The predicted sentence is :')\n",
        "# print(logits_to_sentence(enc_dec_model.predict(input_pad_sentence[index:index+1])[0], output_tokenizer).strip())"
      ],
      "metadata": {
        "id": "HWow0LxlG0A0",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"The Input sentence is: {}\".format(input_data[index]))\n",
        "# print(\"The Output sentence is: {}\".format(output_data[index]))\n",
        "predicted=[]\n",
        "# for a,b,c,d in zip(input_ques_pad_sentence[index:], input_context_pad_sentence[index:],input_ques_emb[index:],input_context_emb[index:]):\n",
        "#   predicted.append(logits_to_sentence(enc_dec_model.predict([[a], [b],[c],[d]])[0],output_tokenizer).strip())\n",
        "for index in range(51035,len(input_pad_sentence)):\n",
        "  # print(index)\n",
        "  predicted.append(logits_to_sentence(enc_dec_model.predict(input_pad_sentence[index:index+1])[0],output_tokenizer))"
      ],
      "metadata": {
        "id": "38dW5VHiNJSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "my_data = {'input':input_data[51035:],\n",
        "        'pred_output':predicted,\n",
        "        'actual_output':output_data[51035:]}\n",
        "df = pd.DataFrame(my_data)\n",
        "df.to_csv(\"/content/drive/MyDrive/DLNLPA2/oldarch_results.csv\")"
      ],
      "metadata": {
        "id": "FfGSYzmFNk_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques=\"What is the principle that states that with sedimentary rocks, inclusions must be older than the formation that contains them?\"\n",
        "context=\"The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock which contains them.\"\n",
        "input_to_cleaner=[ques+sep_token+context]\n",
        "input_to_tokenizer=[clean_sentence(inp) for inp in input_to_cleaner] \n",
        "input_to_padder=input_tokenizer.texts_to_sequences(input_to_tokenizer)\n",
        "test_input = pad_sequences(input_to_padder, max_input_len, padding = \"post\")\n",
        "test_input = test_input.reshape(*test_input.shape, 1)\n",
        "ans=logits_to_sentence(enc_dec_model.predict([test_input[0:1]])[0],output_tokenizer).strip()"
      ],
      "metadata": {
        "id": "BrlBKJ-eVib0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question:\",ques)\n",
        "print(\"Context:\",context)\n",
        "print(\"Answer:\",ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KSeQ_pHWbJ9",
        "outputId": "72a3bff4-e749-4d9d-f851-72dbff236132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the principle that states that with sedimentary rocks, inclusions must be older than the formation that contains them?\n",
            "Context: The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock which contains them.\n",
            "Answer: hora migration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCvHTuRZoe_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}