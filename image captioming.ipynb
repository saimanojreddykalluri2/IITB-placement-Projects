{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   178  100   178    0     0   7416      0 --:--:-- --:--:-- --:--:--  7416\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  5452    0  5452    0     0  50018      0 --:--:-- --:--:-- --:--:-- 50018\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "df = pd.read_csv('csvs/val_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycocotools.coco import COCO\n",
    "# import numpy as np\n",
    "# import skimage.io as io\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pylab\n",
    "# pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_urls=df.coco_url.to_list()\n",
    "# images=[io.imread(image_url) for image_url in image_urls[:100]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"images.txt\", \"w\") as output:\n",
    "#     output.write(str('\\n'.join(set(image_urls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(image_urls))\n",
    "# print(len(set(image_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd image\n",
    "# !wget -i ../images.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.io import imread_collection\n",
    "\n",
    "# #your path \n",
    "# img_dir = '/home/development/sakharamg/Test/image/*.jpg'\n",
    "\n",
    "# #creating a collection with the available images\n",
    "# image_collection = imread_collection(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super(FeatureExtractor, self).__init__()\n",
    "\t\t# Extract VGG-16 Feature Layers\n",
    "    self.features = list(model.features)\n",
    "    self.features = nn.Sequential(*self.features)\n",
    "\t\t# Extract VGG-16 Average Pooling Layer\n",
    "    self.pooling = model.avgpool\n",
    "\t\t# Convert the image into one-dimensional vector\n",
    "    self.flatten = nn.Flatten()\n",
    "\t\t# Extract the first part of fully-connected layer from VGG16\n",
    "    self.fc = model.classifier[0]\n",
    "  \n",
    "  def forward(self, x):\n",
    "\t\t# It will take the input 'x' until it returns the feature vector called 'out'\n",
    "    out = self.features(x)\n",
    "    out = self.pooling(out)\n",
    "    out = self.flatten(out)\n",
    "    out = self.fc(out) \n",
    "    return out \n",
    "\n",
    "# Initialize the model\n",
    "resnet = models.resnet152(pretrained=True)\n",
    "modules = list(resnet.children())[:-1] # delete the last fc layer.\n",
    "resnet = nn.Sequential(*modules)\n",
    "### Now set requires_grad to false\n",
    "for param in resnet.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "# Change the device to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=\"cpu\"\n",
    "# new_model= nn.DataParallel(resnet)\n",
    "new_model = resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.Resize(224), #448\n",
    "    transforms.ToTensor()                              \n",
    "    ])\n",
    "    images = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        img = transform(img)\n",
    "        # print(img.shape)\n",
    "        img = img.reshape(1, 3, 224, 224) \n",
    "        img = img.to(device)\n",
    "        if img is not None:\n",
    "            images[int(filename.split('.')[0])] = img\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_collection = load_images_from_folder('image/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# image_dict={}\n",
    "# for file in images_collection.files:\n",
    "#     image_dict[int(os.path.basename(file).split('.')[0])]=images_collection.load_func(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[]\n",
    "for id in df.image_id:\n",
    "    # print(id)\n",
    "    with torch.no_grad():\n",
    "\t\t# Extract the feature from the image\n",
    "        feature = new_model(images_collection[id])\n",
    "\t    # Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "    features.append(feature.cpu().detach().numpy().reshape(-1))\n",
    "# Convert to NumPy Array\n",
    "\n",
    "    # image_matrix_list.append(images_collection[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# image_matrix_data=np.concatenate( image_matrix_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-mpnet-base-v2')\n",
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_pos_encodings = model.encode(df.caption.to_list())\n",
    "caption_neg_encodings = model.encode(df.negative_captions.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_sums = features.sum(axis=1)\n",
    "features_norm = features / row_sums[:, np.newaxis]\n",
    "\n",
    "\n",
    "row_sums = caption_pos_encodings.sum(axis=1)\n",
    "caption_pos_encodings_norm = caption_pos_encodings / row_sums[:, np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "row_sums = caption_neg_encodings.sum(axis=1)\n",
    "caption_neg_encodings_norm = caption_neg_encodings / row_sums[:, np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "X_correct=np.concatenate((features_norm, caption_pos_encodings_norm), axis=1)\n",
    "X_incorrect=np.concatenate((features_norm, caption_neg_encodings_norm), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_correct=np.ones(len(X_correct))\n",
    "y_incorrect=np.zeros(len(X_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data=np.concatenate((X_correct, X_incorrect), axis=0)\n",
    "y_data=np.concatenate((y_correct, y_incorrect), axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    seed=0\n",
    "    np.random.seed(seed)\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_shuffled,y_shuffled=unison_shuffled_copies(X_data,y_data)\n",
    "from sklearn.utils import shuffle\n",
    "X_shuffled, y_shuffled = shuffle(X_data, y_data, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]-2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_representation = Input(shape=(2048,))\n",
    "image_representation1=Dense(2048, activation='relu')(image_representation)\n",
    "image_representation2=Dense(1024, activation='relu')(image_representation1)\n",
    "image_representation3=Dense(768, activation='relu')(image_representation2)\n",
    "image_representation3=Dense(512, activation='relu')(image_representation3)\n",
    "sentence_representation = Input(shape=(512,))\n",
    "sentence_representation1=Dense(512, activation='relu')(sentence_representation)\n",
    "sentence_representation2=Dense(512, activation='relu')(sentence_representation1)\n",
    "sentence_representation3=Dense(512, activation='relu')(sentence_representation2)\n",
    "\n",
    "merged = Concatenate()([image_representation3,sentence_representation3])\n",
    "merged=Dense(1400, activation='relu')(merged)\n",
    "merged=Dense(1400, activation='relu')(merged)\n",
    "merged=Dense(700, activation='relu')(merged)\n",
    "merged=Dense(100, activation='relu')(merged)\n",
    "merged=Dense(2, activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_model = Model([image_representation,sentence_representation], Activation('softmax')(merged))\n",
    "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
    "            optimizer=Adam(1e-3),\n",
    "            metrics=['accuracy'])\n",
    "enc_dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = enc_dec_model.fit([X_train[:,:2048],X_train[:,2048:]],y_train, batch_size=30, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout=0.2\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(4864, input_dim=len(np.array(X_train)[0]), activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(4864,)),\n",
    "model1.add(Dense(4864, activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(4864,)),\n",
    "model1.add(Dense(2400, activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(2400,)),\n",
    "model1.add(Dense(2400, activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(2400,)),\n",
    "model1.add(Dense(1200, activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(1200,)),\n",
    "model1.add(Dense(1200, activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(1200,)),\n",
    "model1.add(Dense(600, activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(600,)),\n",
    "model1.add(Dense(300, activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(300,)),\n",
    "model1.add(Dense(150, activation='relu'))\n",
    "# layers.Dropout(dropout, input_shape=(150,)),\n",
    "model1.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train,validation_split = 0.1, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score, acc = model1.evaluate(X_test, y_test)\n",
    "# print('Test loss:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/development/sakharamg/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/development/sakharamg/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "49 images found; downloading the remaining 1\n",
      " 100% |██████████████████████| 1/1 [1.4s elapsed, 0s remaining, 0.7 images/s] \n",
      "Writing annotations for 50 downloaded samples to '/home/development/sakharamg/fiftyone/coco-2017/validation/labels.json'\n",
      "Dataset info written to '/home/development/sakharamg/fiftyone/coco-2017/info.json'\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████████| 50/50 [276.3ms elapsed, 0s remaining, 182.3 samples/s]      \n",
      "Dataset 'coco-2017-validation-50' created\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"validation\",\n",
    "    max_samples=50,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        coco-2017-validation-50\n",
       "Media type:  image\n",
       "Num samples: 50\n",
       "Persistent:  False\n",
       "Tags:        ['validation']\n",
       "Sample fields:\n",
       "    id:           fiftyone.core.fields.ObjectIdField\n",
       "    filepath:     fiftyone.core.fields.StringField\n",
       "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
       "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6f9c909a0a5673feaff7d0d273e84ff56438b1a2d4e1b3aff561acaa5aab149"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('openie6': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
